{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02436a3b-60f1-47d9-8820-f4a4dbee93bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23c8104-603d-46d4-9305-6d619053a51f",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "065566ae-a361-4127-acf2-b4096f69cf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' And he went back to meet the fox.',\n",
       " '\"Goodbye,\" he said.',\n",
       " '\"Goodbye,\" said the fox.',\n",
       " '\"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.\"',\n",
       " '\"What is essential is invisible to the eye,\" the little prince repeated, so that he would be sure to remember.',\n",
       " '\"It is the time you have wasted for your rose that makes your rose so important.\"',\n",
       " '\"It is the time I have wasted for my rose --\" said the little prince, so that he would be sure to remember.',\n",
       " '\"Men have forgotten this truth,\" said the fox.',\n",
       " '\"But you must not forget it.',\n",
       " 'You become responsible, forever, for what you have tamed.',\n",
       " 'You are responsible for your rose...\" \"I am responsible for my rose,\" the little prince repeated, so that he would be sure to remember.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "paragraph = \"\"\" And he went back to meet the fox. \"Goodbye,\" he said. \"Goodbye,\" said the fox. \n",
    "\"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" \n",
    "\"What is essential is invisible to the eye,\" the little prince repeated, so that he would be sure to remember. \n",
    "\"It is the time you have wasted for your rose that makes your rose so important.\" \n",
    "\"It is the time I have wasted for my rose --\" said the little prince, so that he would be sure to remember. \"Men have forgotten this truth,\" said the fox. \n",
    "\"But you must not forget it. You become responsible, forever, for what you have tamed. You are responsible for your rose...\" \"I am responsible for my rose,\" the little prince repeated, so that he would be sure to remember. \"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e51d0b3-8f4d-4259-bcd6-d823c406ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And', 'he', 'went', 'back', 'to', 'meet', 'the', 'fox', '.', '``']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e543b-2a91-4b24-b4f4-d293f61a06d5",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1b2d7-9f08-4bfd-9c87-b5c5ceb50301",
   "metadata": {},
   "source": [
    "- Stemming is the process of reducing a word to its base or root form. In NLP, the Natural Language Toolkit (nltk) provides tools for stemming words. The primary purpose of stemming is to remove affixes (such as -ing, -ed, -s) from words to obtain their base form, which helps in text processing tasks like search and indexing.\n",
    "\n",
    "\n",
    "| Word          | Stemmed Word  |\n",
    "|---------------|----------------|\n",
    "| running       | run            |\n",
    "| jumped        | jump           |\n",
    "| happily       | happi          |\n",
    "| studies       | studi          |\n",
    "| better        | better         |\n",
    "| swimming      | swim           |\n",
    "| dancing       | danc           |\n",
    "\n",
    "\n",
    "\n",
    "Here's an example of how to perform stemming using the Porter stemmer in nltk:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# List of words to stem\n",
    "words = [\"running\", \"jumped\", \"happily\", \"studies\", \"better\", \"swimming\", \"dancing\"]\n",
    "\n",
    "# Stem the words and print the results\n",
    "stemmed_words = [(word, stemmer.stem(word)) for word in words]\n",
    "for word, stemmed_word in stemmed_words:\n",
    "    print(f\"{word} -> {stemmed_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f5c4e-6947-4841-8bf6-006e6cdb6a2f",
   "metadata": {},
   "source": [
    "The following are the functions you can use when stemming:\n",
    "\n",
    "**Functions (NLTK Stemming)**  \n",
    "\n",
    "| Function           | Description                                                                 | Syntax                                    |\n",
    "|-------------------|---------------------------------------------------------------------------|------------------------------------------|\n",
    "| `PorterStemmer`   | Initializes the Porter Stemmer, which is a rule-based algorithm for removing common suffixes from words in English. It is less aggressive compared to other stemmers. | `stemmer = PorterStemmer()`            |\n",
    "| `LancasterStemmer` | Initializes the Lancaster Stemmer, which is an aggressive stemming algorithm that reduces words to a very short root form, often cutting off more letters than necessary. | `stemmer = LancasterStemmer()`         |\n",
    "| `SnowballStemmer`  | Initializes the Snowball Stemmer, which is an improved version of the Porter Stemmer. It supports multiple languages and applies more refined stemming rules. | `stemmer = SnowballStemmer('english')` |\n",
    "| `stem`            | Reduces a given word to its root form using the selected stemmer. This method applies the stemming rules defined by the respective stemmer. | `stemmer.stem('running')`              |\n",
    "\n",
    "\n",
    "\n",
    "**Arguments (NLTK Stemming)**  \n",
    "\n",
    "| Argument      | Description                                                                 | Possible Values         | Syntax                                  |\n",
    "|--------------|---------------------------------------------------------------------------|-------------------------|-----------------------------------------|\n",
    "| `language`   | Specifies the language for the SnowballStemmer, allowing stemming in multiple languages like English, French, and German. | `'english'`, `'french'`, `'german'` | `SnowballStemmer(language='english')` |\n",
    "| `word`       | The input word to be stemmed, which will be reduced to its base/root form. | Any string              | `stemmer.stem('playing')`              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8829962-0cc0-4367-b682-b41db6fec16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'he', 'went', 'back', 'to', 'meet', 'the', 'fox', '.', '``', 'goodby', ',', \"''\", 'he', 'said', '.', '``', 'goodby', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'and', 'now', 'here', 'is', 'my', 'secret', ',', 'a', 'veri', 'simpl', 'secret', ':', 'it', 'is', 'onli', 'with', 'the', 'heart', 'that', 'one', 'can', 'see', 'rightli', ';', 'what', 'is', 'essenti', 'is', 'invis', 'to', 'the', 'eye', '.', \"''\", '``', 'what', 'is', 'essenti', 'is', 'invis', 'to', 'the', 'eye', ',', \"''\", 'the', 'littl', 'princ', 'repeat', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'rememb', '.', '``', 'it', 'is', 'the', 'time', 'you', 'have', 'wast', 'for', 'your', 'rose', 'that', 'make', 'your', 'rose', 'so', 'import', '.', \"''\", '``', 'it', 'is', 'the', 'time', 'i', 'have', 'wast', 'for', 'my', 'rose', '--', \"''\", 'said', 'the', 'littl', 'princ', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'rememb', '.', '``', 'men', 'have', 'forgotten', 'thi', 'truth', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'but', 'you', 'must', 'not', 'forget', 'it', '.', 'you', 'becom', 'respons', ',', 'forev', ',', 'for', 'what', 'you', 'have', 'tame', '.', 'you', 'are', 'respons', 'for', 'your', 'rose', '...', \"''\", '``', 'i', 'am', 'respons', 'for', 'my', 'rose', ',', \"''\", 'the', 'littl', 'princ', 'repeat', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'rememb', '.']\n",
      "\n",
      "\n",
      "['And', 'he', 'went', 'back', 'to', 'meet', 'the', 'fox', '.', '``', 'Goodbye', ',', \"''\", 'he', 'said', '.', '``', 'Goodbye', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'And', 'now', 'here', 'is', 'my', 'secret', ',', 'a', 'very', 'simple', 'secret', ':', 'It', 'is', 'only', 'with', 'the', 'heart', 'that', 'one', 'can', 'see', 'rightly', ';', 'what', 'is', 'essential', 'is', 'invisible', 'to', 'the', 'eye', '.', \"''\", '``', 'What', 'is', 'essential', 'is', 'invisible', 'to', 'the', 'eye', ',', \"''\", 'the', 'little', 'prince', 'repeated', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.', '``', 'It', 'is', 'the', 'time', 'you', 'have', 'wasted', 'for', 'your', 'rose', 'that', 'makes', 'your', 'rose', 'so', 'important', '.', \"''\", '``', 'It', 'is', 'the', 'time', 'I', 'have', 'wasted', 'for', 'my', 'rose', '--', \"''\", 'said', 'the', 'little', 'prince', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.', '``', 'Men', 'have', 'forgotten', 'this', 'truth', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'But', 'you', 'must', 'not', 'forget', 'it', '.', 'You', 'become', 'responsible', ',', 'forever', ',', 'for', 'what', 'you', 'have', 'tamed', '.', 'You', 'are', 'responsible', 'for', 'your', 'rose', '...', \"''\", '``', 'I', 'am', 'responsible', 'for', 'my', 'rose', ',', \"''\", 'the', 'little', 'prince', 'repeated', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmedWords = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmedWords)\n",
    "print(\"\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268f8dc-a4aa-49ea-bd77-31e556f98d74",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51116ba-afa1-4a2f-a35e-d82bd498b216",
   "metadata": {},
   "source": [
    "- Stop words are commonly used words (such as \"the\", \"is\", \"in\", \"and\", etc.) that are often ignored in text processing tasks because they carry very little meaningful information. In NLP, removing stop words can help reduce the size of the dataset and improve the performance of text analysis algorithms by focusing on the more significant words.\n",
    "\n",
    "- The Natural Language Toolkit (nltk) provides a list of stop words for various languages, and it includes functions to filter out these words from text.\n",
    "\n",
    "Here's an example of how to use stop words in nltk:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example to demonstrate the removal of stop words.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Filtered Text:\", \" \".join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "654f5cdc-6518-4d19-8540-adfbe9f8294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'went', 'back', 'meet', 'fox', '.', '``', 'goodby', ',', \"''\", 'said', '.', '``', 'goodby', ',', \"''\", 'said', 'fox', '.', '``', 'and', 'secret', ',', 'simpl', 'secret', ':', 'it', 'heart', 'one', 'see', 'rightli', ';', 'essenti', 'invis', 'eye', '.', \"''\", '``', 'what', 'essenti', 'invis', 'eye', ',', \"''\", 'littl', 'princ', 'repeat', ',', 'would', 'sure', 'rememb', '.', '``', 'it', 'time', 'wast', 'rose', 'make', 'rose', 'import', '.', \"''\", '``', 'it', 'time', 'i', 'wast', 'rose', '--', \"''\", 'said', 'littl', 'princ', ',', 'would', 'sure', 'rememb', '.', '``', 'men', 'forgotten', 'truth', ',', \"''\", 'said', 'fox', '.', '``', 'but', 'must', 'forget', '.', 'you', 'becom', 'respons', ',', 'forev', ',', 'tame', '.', 'you', 'respons', 'rose', '...', \"''\", '``', 'i', 'respons', 'rose', ',', \"''\", 'littl', 'princ', 'repeat', ',', 'would', 'sure', 'rememb', '.']\n",
      "\n",
      "\n",
      "['And', 'he', 'went', 'back', 'to', 'meet', 'the', 'fox', '.', '``', 'Goodbye', ',', \"''\", 'he', 'said', '.', '``', 'Goodbye', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'And', 'now', 'here', 'is', 'my', 'secret', ',', 'a', 'very', 'simple', 'secret', ':', 'It', 'is', 'only', 'with', 'the', 'heart', 'that', 'one', 'can', 'see', 'rightly', ';', 'what', 'is', 'essential', 'is', 'invisible', 'to', 'the', 'eye', '.', \"''\", '``', 'What', 'is', 'essential', 'is', 'invisible', 'to', 'the', 'eye', ',', \"''\", 'the', 'little', 'prince', 'repeated', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.', '``', 'It', 'is', 'the', 'time', 'you', 'have', 'wasted', 'for', 'your', 'rose', 'that', 'makes', 'your', 'rose', 'so', 'important', '.', \"''\", '``', 'It', 'is', 'the', 'time', 'I', 'have', 'wasted', 'for', 'my', 'rose', '--', \"''\", 'said', 'the', 'little', 'prince', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.', '``', 'Men', 'have', 'forgotten', 'this', 'truth', ',', \"''\", 'said', 'the', 'fox', '.', '``', 'But', 'you', 'must', 'not', 'forget', 'it', '.', 'You', 'become', 'responsible', ',', 'forever', ',', 'for', 'what', 'you', 'have', 'tamed', '.', 'You', 'are', 'responsible', 'for', 'your', 'rose', '...', \"''\", '``', 'I', 'am', 'responsible', 'for', 'my', 'rose', ',', \"''\", 'the', 'little', 'prince', 'repeated', ',', 'so', 'that', 'he', 'would', 'be', 'sure', 'to', 'remember', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming The Little Prince and using stopwords\n",
    "# as you can see from the top example, the unnecessary words were also process, we use stopwords to handle that\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "impStemmedWords = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "print(impStemmedWords)\n",
    "print(\"\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "daead2fd-6ca3-4c69-b00c-daf11f44546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and went back meet fox .',\n",
       " \"`` goodby , '' said .\",\n",
       " \"`` goodby , '' said fox .\",\n",
       " \"`` and secret , simpl secret : it heart one see rightli ; essenti invis eye . ''\",\n",
       " \"`` what essenti invis eye , '' littl princ repeat , would sure rememb .\",\n",
       " \"`` it time wast rose make rose import . ''\",\n",
       " \"`` it time i wast rose -- '' said littl princ , would sure rememb .\",\n",
       " \"`` men forgotten truth , '' said fox .\",\n",
       " '`` but must forget .',\n",
       " 'you becom respons , forev , tame .',\n",
       " \"you respons rose ... '' `` i respons rose , '' littl princ repeat , would sure rememb .\"]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmedSentences = []\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    impStemmedWords = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    stemmedSentences.append(' '.join(impStemmedWords))\n",
    "stemmedSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e355ee3-25b8-429f-8e70-940485420d0a",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "---\n",
    "- Does the same thing as stemming but it transforms the word to a meaningful word:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5793d-79c7-4d7f-a7ea-baaa20175964",
   "metadata": {},
   "source": [
    "\n",
    "| Word          | Lemmatized Word  |\n",
    "|---------------|------------------|\n",
    "| running       | run              |\n",
    "| jumped        | jump             |\n",
    "| happily       | happily          |\n",
    "| studies       | study            |\n",
    "| better        | good             |\n",
    "| swimming      | swim             |\n",
    "| dancing       | dance            |\n",
    "\n",
    "\n",
    "Here's an example of how to perform lemmatization using the WordNetLemmatizer in nltk:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List of words to lemmatize\n",
    "words = [\"running\", \"jumped\", \"happily\", \"studies\", \"better\", \"swimming\", \"dancing\"]\n",
    "\n",
    "# Lemmatize the words and print the results\n",
    "lemmatized_words = [(word, lemmatizer.lemmatize(word)) for word in words]\n",
    "for word, lemmatized_word in lemmatized_words:\n",
    "    print(f\"{word} -> {lemmatized_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a6f2f-b7de-44b3-8766-ed3a0f29c731",
   "metadata": {},
   "source": [
    "The following are the functions and arguments you can use when doing lemmatization\n",
    "**Functions (NLTK Lemmatization)**  \n",
    "\n",
    "| Function          | Description                                                                 | Syntax                                  |\n",
    "|------------------|---------------------------------------------------------------------------|-----------------------------------------|\n",
    "| `WordNetLemmatizer` | Initializes the WordNet Lemmatizer, which reduces words to their dictionary form using lexical databases like WordNet. | `lemmatizer = WordNetLemmatizer()` |\n",
    "| `lemmatize`      | Converts a word to its base form (lemma) while considering its part of speech (POS). If no POS is provided, it defaults to **noun**. | `lemmatizer.lemmatize('running', pos='v')` |\n",
    "\n",
    "**Arguments (NLTK Lemmatization)**  \n",
    "\n",
    "| Argument   | Description                                                                            | Possible Values               | Syntax                                    |\n",
    "|-----------|--------------------------------------------------------------------------------------|-------------------------------|------------------------------------------|\n",
    "| `word`    | The input word to be lemmatized, which will be transformed into its dictionary form. | Any string                     | `lemmatizer.lemmatize('wolves')`       |\n",
    "| `pos`     | Specifies the part of speech (POS) of the word. If not provided, it defaults to a noun. Providing the correct POS improves accuracy. | `'n'` (noun), `'v'` (verb), `'a'` (adjective), `'r'` (adverb) | `lemmatizer.lemmatize('better', pos='a')` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b68b880b-adde-4b1b-be7f-78f2cca69948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' And he went back to meet the fox.',\n",
       " '\"Goodbye,\" he said.',\n",
       " '\"Goodbye,\" said the fox.',\n",
       " '\"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.\"',\n",
       " '\"What is essential is invisible to the eye,\" the little prince repeated, so that he would be sure to remember.',\n",
       " '\"It is the time you have wasted for your rose that makes your rose so important.\"',\n",
       " '\"It is the time I have wasted for my rose --\" said the little prince, so that he would be sure to remember.',\n",
       " '\"Men have forgotten this truth,\" said the fox.',\n",
       " '\"But you must not forget it.',\n",
       " 'You become responsible, forever, for what you have tamed.',\n",
       " 'You are responsible for your rose...\" \"I am responsible for my rose,\" the little prince repeated, so that he would be sure to remember.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2629c866-51ab-459e-bf95-d4cc0a7dd7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And went back meet fox .', \"`` Goodbye , '' said .\", \"`` Goodbye , '' said fox .\", \"`` And secret , simple secret : It heart one see rightly ; essential invisible eye . ''\", \"`` What essential invisible eye , '' little prince repeated , would sure remember .\", \"`` It time wasted rose make rose important . ''\", \"`` It time I wasted rose -- '' said little prince , would sure remember .\", \"`` Men forgotten truth , '' said fox .\", '`` But must forget .', 'You become responsible , forever , tamed .', \"You responsible rose ... '' `` I responsible rose , '' little prince repeated , would sure remember .\"]\n",
      "\n",
      "\n",
      "[' And he went back to meet the fox.', '\"Goodbye,\" he said.', '\"Goodbye,\" said the fox.', '\"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.\"', '\"What is essential is invisible to the eye,\" the little prince repeated, so that he would be sure to remember.', '\"It is the time you have wasted for your rose that makes your rose so important.\"', '\"It is the time I have wasted for my rose --\" said the little prince, so that he would be sure to remember.', '\"Men have forgotten this truth,\" said the fox.', '\"But you must not forget it.', 'You become responsible, forever, for what you have tamed.', 'You are responsible for your rose...\" \"I am responsible for my rose,\" the little prince repeated, so that he would be sure to remember.']\n"
     ]
    }
   ],
   "source": [
    "# now what you want to do is to transform the sentences list to be a list of lemmatized words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizedSentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    lemmatizedSentences.append(' '.join(words))\n",
    "print(lemmatizedSentences)\n",
    "print(\"\\n\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbad56-b054-428e-afb3-399ae4a98a6f",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840222b-4edb-4a36-8c7a-d86a623001cf",
   "metadata": {},
   "source": [
    "Bag of words is representing the words into a dataframe really for example you have the following sentences:\n",
    "- He is a good boy boy\n",
    "- She is a good girl\n",
    "- Boy and girl are good\n",
    "\n",
    "We can apply stopwords and lemmatizer to transform them to tokens. If we do the output will be:\n",
    "- good boy boy\n",
    "- good girl\n",
    "- boy girl good\n",
    "\n",
    "After that we get the count plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d01bfba0-5e6d-4219-9107-7ae974823b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ['good', 'boy', 'girl']\n",
    "y = [3, 3, 2]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90c5f2-b478-4857-a5cc-8ee179ccadb8",
   "metadata": {},
   "source": [
    "And then we make them a vector:\n",
    "\n",
    "<table class=\"table_class_basic_full_width\" style=\"font-size: 85%; width: 100%; border-collapse: collapse; border: 1px solid;\">\r\n",
    "                <thead><tr><th style=\"width:20%;  padding: 2px; border: 1px solid;\">​</th><th style=\"width:20%;  padding: 2px; border: 1px solid;\">good​</th><th style=\"width:20%;  padding: 2px; border: 1px solid;\">boy​</th><th style=\"width:20%;  padding: 2px; border: 1px solid;\">​girl</th><th style=\"width:20%;  padding: 2px; border: 1px solid;\">output​</th></tr></thead>\r\n",
    "                <tbody><tr><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​sentence 1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​2</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​0</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​x</td></tr><tr><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​sentence 2</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​0</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​x</td></tr><tr><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​sentence 3</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​1</td><td style=\"width:20%;  padding: 2px; border: 1px solid;\">​y</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f96a3-ee54-4134-a828-3704e5ebf5ae",
   "metadata": {},
   "source": [
    "remember that this can be vectores if you do the `.value` on the input and output column/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1733783b-799e-4757-9bdd-876552c08559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little prince tamed fox',\n",
       " 'hour departure drew near ah said fox shall cry',\n",
       " 'fault said little prince',\n",
       " 'never wished sort harm wanted tame yes said fox',\n",
       " 'going cry',\n",
       " 'said little prince',\n",
       " 'yes said fox',\n",
       " 'done good',\n",
       " 'done good said fox color wheat field',\n",
       " 'added go look rose',\n",
       " 'understand unique world',\n",
       " 'come back say goodbye make present secret']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming that the needed things are imported\n",
    "import re\n",
    "\n",
    "paragraph2 = \"\"\"So the little prince tamed the fox. And when the hour of his departure drew near -- \n",
    "\"Ah,\" said the fox, \"I shall cry.\"\n",
    "\"It is your own fault,\" said the little prince. \"I never wished you any sort of harm; but you wanted me to tame you...\"\n",
    "\"Yes, that is so,\" said the fox. \n",
    "\"But now you are going to cry!\" said the little prince. \n",
    "\"Yes, that is so,\" said the fox. \"Then it has done you no good at all!\"\n",
    "\"It has done me good,\" said the fox, \"because of the color of the wheat fields.\" And then he added: \"Go and look again at the roses. \n",
    "You will understand now that yours is unique in all the world. \n",
    "Then come back to say goodbye to me, and I will make you a present of a secret.\" \"\"\"\n",
    "\n",
    "# lowercase the words so that words won't be repeated and erase unneccesary characters\n",
    "\n",
    "paragraph2 = paragraph2.lower()\n",
    "\n",
    "# Tokenize the paragraph to sentences\n",
    "sentences2 = nltk.sent_tokenize(paragraph2)\n",
    "\n",
    "# Lemmatize the sentences\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizedSentences2 = []\n",
    "\n",
    "for i in range(len(sentences2)):\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", ' ', sentences2[i])\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    lemmatizedSentences2.append(\" \".join(words))\n",
    "lemmatizedSentences2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54572d5c-b1eb-470a-a844-1adb51ab3a76",
   "metadata": {},
   "source": [
    "You can then vectorise this with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5486382c-36d3-4646-a922-088a862b2d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1\n",
      "  0 1 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      "  0 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "(12, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(lemmatizedSentences2)\n",
    "print(X.toarray())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bc530-09bd-4c08-9296-ef1ede149e9f",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b037fe-7e76-44b3-aabf-391a258704ce",
   "metadata": {},
   "source": [
    "Almost the same as Bag of Words, the difference is that TF-IDF gives weight to each word instead of just count.\n",
    "\n",
    "Bag of words is representing the words into a dataframe really for example you have the following sentences:\n",
    "- He is a good boy boy\n",
    "- She is a good girl\n",
    "- Boy and girl are good\n",
    "\n",
    "We can apply stopwords and lemmatizer to transform them to tokens. If we do the output will be:\n",
    "- good boy boy\n",
    "- good girl\n",
    "- boy girl good\n",
    "\n",
    "Remember that we have these sentences now we'll transform this to Vectors but with weight instead of count. So the first thing we do is find the TF or the `Term Frequency` and that is achieved using the formula:\n",
    "\n",
    "$$TF = \\dfrac{\\text{Number of occurence of the word}}{\\text{total number of words}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe06768-f1ea-4962-8c71-c9ff3215a9fc",
   "metadata": {},
   "source": [
    "**Term Frequency**\n",
    "\n",
    "| Documents  | boy | girl | good | \n",
    "|-------|------------|------------|------------|\n",
    "| Document 1  | 2/3        | 0        | 1/3        |\n",
    "| Document 2 | 0          | 1/2          | 1/2          |   \n",
    "| Document 3 | 1/3          | 1/3       | 1/3          | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59838e-bb32-4374-834a-eb6bacec2c8c",
   "metadata": {},
   "source": [
    "After that we get the `IDF` which is when you divide the total number of documents in the corpus by the number of documents that contain the term and the get its logarithm. Now it is to be remembered that each word have its own `IDF`\n",
    "\n",
    "$$ IDF = \\log{(\\frac{\\text{number of documents}}{\\text{number of document that contain the term}})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4525f-3478-499f-a5fd-23ad5c67e89b",
   "metadata": {},
   "source": [
    "**IDF**\n",
    "\n",
    "**Term Frequency**\n",
    "\n",
    "| Words  | IDF | \n",
    "|-------|------------|\n",
    "| boy  | $log(3/2)$        |\n",
    "| girl | $log(3/2)$          | \n",
    "| good | $log(3/3)$          | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad5e3d-e0c2-41e3-88d7-2b3a5e2be60c",
   "metadata": {},
   "source": [
    "And then given these two values, we get the matrix by multiplying the `TF`to their respective `IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b22f3d-07e4-47fa-af69-90fc42490cfc",
   "metadata": {},
   "source": [
    "and then we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ccf28-4c28-4963-8b75-96f9add98d42",
   "metadata": {},
   "source": [
    "**TF-IDF**\r\n",
    "\r\n",
    "| Documents  | boy | girl | good | Output |\r\n",
    "|------------|-----|------|------|--------|\r\n",
    "| Document 1 | 2/3 $\\times \\log(3/2)$ | $0 \\times \\log(3/2)$ | 1/3 $\\times \\log(3/3)$ |          |\r\n",
    "| Document 2 | 0 $\\times \\log(3/2)$   | 1/2 $\\times \\log(3/2)$ | 1/2 $\\times \\log(3/3)$ |          |\r\n",
    "| Document 3 | 1/3 $\\times \\log(3/2)$ | 1/3 $\\times \\log(3/2)$ | 1/3 $\\times \\log(3/3)$ |          |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535b6a3-ba9f-4604-84be-7af118347b11",
   "metadata": {},
   "source": [
    "Now from that, we get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7198c2de-5818-4892-81c3-0b29514edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so now given this paragraph two we transform it so that machine can understand it\n",
    "\n",
    "paragraph2 = \"\"\"So the little prince tamed the fox. And when the hour of his departure drew near -- \n",
    "\"Ah,\" said the fox, \"I shall cry.\"\n",
    "\"It is your own fault,\" said the little prince. \"I never wished you any sort of harm; but you wanted me to tame you...\"\n",
    "\"Yes, that is so,\" said the fox. \n",
    "\"But now you are going to cry!\" said the little prince. \n",
    "\"Yes, that is so,\" said the fox. \"Then it has done you no good at all!\"\n",
    "\"It has done me good,\" said the fox, \"because of the color of the wheat fields.\" And then he added: \"Go and look again at the roses. \n",
    "You will understand now that yours is unique in all the world. \n",
    "Then come back to say goodbye to me, and I will make you a present of a secret.\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a01415b-b223-4ce2-8cff-c12e16386740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so the little prince tamed the fox.',\n",
       " 'and when the hour of his departure drew near -- \\n\"ah,\" said the fox, \"i shall cry.\"',\n",
       " '\"it is your own fault,\" said the little prince.',\n",
       " '\"i never wished you any sort of harm; but you wanted me to tame you...\"\\n\"yes, that is so,\" said the fox.',\n",
       " '\"but now you are going to cry!\"',\n",
       " 'said the little prince.',\n",
       " '\"yes, that is so,\" said the fox.',\n",
       " '\"then it has done you no good at all!\"',\n",
       " '\"it has done me good,\" said the fox, \"because of the color of the wheat fields.\"',\n",
       " 'and then he added: \"go and look again at the roses.',\n",
       " 'you will understand now that yours is unique in all the world.',\n",
       " 'then come back to say goodbye to me, and i will make you a present of a secret.\"']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the paragraph, lowercase so no duplicated words\n",
    "paragraph2 = paragraph2.lower()\n",
    "\n",
    "# Tokenize the paragraph into sentences, your goal is to have a list of sentences (lemmatized)\n",
    "sentences = nltk.sent_tokenize(paragraph2)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "efef664a-c9cd-4aad-8b89-5e1a1c1b7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little prince tamed fox', 'hour departure drew near ah said fox shall cry', 'fault said little prince', 'never wished sort harm wanted tame yes said fox', 'going cry', 'said little prince', 'yes said fox', 'done good', 'done good said fox color wheat field', 'added go look rose', 'understand unique world', 'come back say goodbye make present secret']\n",
      "\n",
      "\n",
      "['so the little prince tamed the fox.', 'and when the hour of his departure drew near -- \\n\"ah,\" said the fox, \"i shall cry.\"', '\"it is your own fault,\" said the little prince.', '\"i never wished you any sort of harm; but you wanted me to tame you...\"\\n\"yes, that is so,\" said the fox.', '\"but now you are going to cry!\"', 'said the little prince.', '\"yes, that is so,\" said the fox.', '\"then it has done you no good at all!\"', '\"it has done me good,\" said the fox, \"because of the color of the wheat fields.\"', 'and then he added: \"go and look again at the roses.', 'you will understand now that yours is unique in all the world.', 'then come back to say goodbye to me, and i will make you a present of a secret.\"']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "import re\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences2 = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", ' ', sentences[i])\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in sentence if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences2.append(' '.join(words))\n",
    "\n",
    "print(sentences2)\n",
    "print(\"\\n\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6857749f-9097-4d32-b573-6bdc3185a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38801002\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47673404 0.         0.         0.         0.         0.\n",
      "  0.47673404 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.62840874 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.36670065 0.         0.         0.         0.31492678\n",
      "  0.36670065 0.         0.36670065 0.         0.         0.22641876\n",
      "  0.         0.         0.         0.         0.         0.36670065\n",
      "  0.         0.         0.         0.36670065 0.         0.\n",
      "  0.         0.         0.20673525 0.         0.         0.36670065\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.63642679 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.48281682 0.         0.         0.         0.         0.\n",
      "  0.48281682 0.         0.35879906 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22641876\n",
      "  0.         0.         0.         0.         0.36670065 0.\n",
      "  0.         0.         0.         0.         0.36670065 0.\n",
      "  0.         0.         0.20673525 0.         0.         0.\n",
      "  0.36670065 0.36670065 0.         0.         0.         0.36670065\n",
      "  0.         0.36670065 0.         0.31492678]\n",
      " [0.         0.         0.         0.         0.         0.65152087\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75863071 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.62594787 0.         0.         0.         0.         0.\n",
      "  0.62594787 0.         0.46516505 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.51514212\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4703587  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.71651328]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.43962116 0.         0.\n",
      "  0.         0.37755176 0.         0.         0.43962116 0.27144342\n",
      "  0.         0.         0.37755176 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.24784573 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.43962116 0.         0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.57735027 0.\n",
      "  0.         0.         0.57735027 0.        ]\n",
      " [0.         0.         0.37796447 0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.37796447\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "(12, 40)\n"
     ]
    }
   ],
   "source": [
    "# so now that we have these list of sentences, we can proceed with to transforming it to TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(sentences2).toarray()\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab007c3-2df5-4d31-8d26-7b3f1d5c7ce3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
