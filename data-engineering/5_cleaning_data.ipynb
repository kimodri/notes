{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dc6e0a-d782-4faa-9a2a-f2083c06f8df",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n",
    "\n",
    "- Diagnose dirty data\n",
    "- Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1b9e3-1102-4ec3-911d-37897fbf1b52",
   "metadata": {},
   "source": [
    "## Data Type Constraints\n",
    "- We gotta make sure that we are working with the right data type\n",
    "- So we will be working with conversions here\n",
    "\n",
    "1. To convert string to int we use `.str` to columns:\n",
    "\n",
    "        `df['col'].str.strip/replace()`\n",
    "        \n",
    "        `df['col'].astype('int')`\n",
    "        \n",
    "        **IF** you want to verify that revenue is now a specific data type (`int`), you can do:\n",
    "        \n",
    "        `assert df['col'].dtype == 'int'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810c07c-c337-46f2-81a9-25ce6d368065",
   "metadata": {},
   "source": [
    "2. To make an integer a category/string, we stil use `.astype('category')`\n",
    "   - We need to do this because when we do `.describe` it calculates the wrong summary stats\n",
    "```python\n",
    "df['col_int'] = df['col_int'].astype('category')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6eea1-93ac-44e1-92ec-18d836bb1992",
   "metadata": {},
   "source": [
    "## Data Range Constraints\n",
    "\n",
    "To deal out of range data:\n",
    "- we drop them\n",
    "- setting custom minimums and maximums\n",
    "- treat as missing and impute it\n",
    "- setting custom value depending on business assumptions\n",
    "\n",
    "One good syntax for assigning a value to out of ranges is \n",
    "\n",
    "`df.loc[df['col'] > 5, 'col'] = 5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e168e8-2225-4413-bbac-3fcb0dc340cb",
   "metadata": {},
   "source": [
    "1. Dropping them:\n",
    "```python\n",
    "df.drop(df[condition].index, inplace = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cf9d3-2bc1-4256-85aa-54d9af730f57",
   "metadata": {},
   "source": [
    "### Working with Date Ranges\n",
    "- To check for date ranges what you do is get the today's date and check if anything surpasses it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937de475-c0c7-4480-b1ec-d7a4ca4f7ca2",
   "metadata": {},
   "source": [
    "## Uniqueness Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31288748-8231-41d0-947d-d1bd76bac12b",
   "metadata": {},
   "source": [
    "The `df.duplicated()` method in Pandas is used to identify duplicate rows within a DataFrame.\n",
    "\n",
    "**Important Consideration:**\n",
    "\n",
    "By default, `df.duplicated()` marks all occurrences *after* the first one as `True`. This means the *first* instance of a duplicate set will be marked as `False`, which can sometimes be misleading if you want to see *all* parts of a duplicate set.\n",
    "\n",
    "**Enhancing Duplicate Detection with Arguments:**\n",
    "\n",
    "To achieve more precise control over duplicate identification, utilize the following arguments:\n",
    "\n",
    "* **`subset`**: This argument accepts a list of column names. When provided, the duplication check will only be performed on the specified columns. This is useful when you want to define a duplicate based on a subset of your data rather than the entire row.\n",
    "\n",
    "* **`keep`**: This argument controls which duplicate observations (if any) are marked as `False` (not a duplicate) or `True` (a duplicate). It accepts three values:\n",
    "    * `'first'` (default): Marks all duplicates as `True` except for the first occurrence.\n",
    "    * `'last'`: Marks all duplicates as `True` except for the last occurrence.\n",
    "    * `False`: Marks *all* occurrences of duplicate rows as `True`, including the first and last. This is often the preferred setting when you want to identify every single row that is part of a duplicate set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3ea90-c3c4-47c1-9117-c2c6d0fe3ac0",
   "metadata": {},
   "source": [
    "#### Treating Duplicates\n",
    "There are two types of duplicates and each one require a specific strategy:\n",
    "\n",
    "1. Complete duplicates: All columns are duplicated\n",
    "2. Duplicates with discrepancy: Only some columns are duplicated\n",
    "\n",
    "To deal with the **first one**:\n",
    "- Only keep one of them\n",
    "- You can use the `.drop_duplicates`\n",
    "  - This has the same arguments as the `.duplicated()`\n",
    "\n",
    "```python\n",
    "# now remember that firt is the default so the next occurences only will be dropped\n",
    "df.drop_duplicates(inplace = True)\n",
    "```\n",
    "\n",
    "To deal with the **second one**:\n",
    "- For example there are only discrepancy with the heigh, what you can do is use stat methods based on your understanding of the attributes/fields (You can get the mean, max, etc)\n",
    "- The `.groupby()` and `.agg()` is actually used for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae609f27-2eb5-4246-983d-860adb5cd2ff",
   "metadata": {},
   "source": [
    "For example, if you are dealing with people's height and weight and there are duplicates with the `name`, `address` but differences with the `height` and `weight`, you can do:\n",
    "\n",
    "```python\n",
    "# cols to group by\n",
    "cols = ['names', 'address']\n",
    "\n",
    "# sum stat\n",
    "summaries = {'height': 'max', 'weight': 'mean'}\n",
    "\n",
    "df = df.groupby(by = cols).agg(summaries).reset_index() \n",
    "```\n",
    "\n",
    "Basically, what happens here is you grouped all the rows, now given that most rows are unique they would not change even if you subjected it to the sum stat, only those with dupicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb330299-e1c1-4e7c-9441-bbf9c3926608",
   "metadata": {},
   "source": [
    "## II. Membership Constraints (Categorical Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eab0b9-edea-4f88-b0ef-a2fd33aaa7d6",
   "metadata": {},
   "source": [
    "### Range Inconsistency\n",
    "How do you find inconsistent data (out of the category)\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "0. Check first if there are inconsistencies using the `.unique()`\n",
    "\n",
    "1. Get the set difference from the data with incosistency to the categorical data\n",
    "\n",
    "```python\n",
    "# this will return a set whose values are in set A but not in set B\n",
    "inconsisten_categories = set(current_df['cat_col']).difference(cat_df['cat_col_complete'])\n",
    "```\n",
    "2. Now, you want to know what are the rows that has this elements of the resulting set from the previous step using the `.isin()` and subset the current dataframe\n",
    "\n",
    "```python\n",
    "inconsisten_rows = current_df['cat_col'].isin(inconsistent_categories)\n",
    "current_df[inconsisten_rows]\n",
    "```\n",
    "3. Work on it\n",
    "   - You can drop it by simply using `~`: `current_df[~inconsistent_rows]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fcf3b-e4c5-4d2d-9409-dee80fba2000",
   "metadata": {},
   "source": [
    "### Collapsing too many categories to Few\n",
    "\n",
    "Let's take for example, we are woking on incomes and we want to categorize incomes to ranges, you can use `cut()` for it (inclusivity, `(x, y]`):\n",
    "\n",
    "**Ranges:**\n",
    "```python\n",
    "ranges = [0, 200000, 500000, np.inf]\n",
    "\n",
    "group_names = ['0-200k', '200k-500k', '500k+']\n",
    "\n",
    "df['new_cat_col'] = pd.cut(df['income'], bins = ranges, labels = group_names)\n",
    "```\n",
    "\n",
    "Sometimes, we may want to make our categories fewer, for example if we have categories: `['Microsoft', 'MacOS', 'IOS' ...]` and you want to just be `['DesktopOS', 'MobileOS']`\n",
    "\n",
    "To do that we can use the `.replace()`\n",
    "\n",
    "```python\n",
    "mapping = {'MicrosoftOS': 'DesktopOS', 'Linux': 'DesktopOS', 'IOS': 'MobileOS', ...}\n",
    "df['os'] = df['os'].replace(mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24fb2a-42b6-4d05-8f10-be47a560cda6",
   "metadata": {},
   "source": [
    "### Cleaning Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071011cc-9bff-4c7f-a4c2-051903bf9f9c",
   "metadata": {},
   "source": [
    "This is just using regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4a7ae-9de8-43cd-992d-61b8a859f15c",
   "metadata": {},
   "source": [
    "For example, if you have a column with phone numnbers `['+63976-202-5431', ..., '45643']`\n",
    "\n",
    "and what you want is to have a `09762025431` and remove number whose length is not 11, you can use the `.str.replace()`\n",
    "\n",
    "```python\n",
    "df['phone_numbers'] = df['phone_numbers'].str.replace('+63', '0').str.replace('-', '')\n",
    "\n",
    "df.loc[df['phone_numbers'].str.len() < 11, 'phone_numbers']] = np.nan\n",
    "```\n",
    "**Assertion with text data:**\n",
    "\n",
    "You can perform assertion as well like ths:\n",
    "\n",
    "```python\n",
    "sanity_check = df['phone_numbers'].str.len() \n",
    "\n",
    "assert sanity_check.min() >= 11\n",
    "\n",
    "assert df['phone_numbers'].str.contains(\"+63|-\").any() == False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26452c7a-664a-45f6-807e-dea1907a7dae",
   "metadata": {},
   "source": [
    "## III. Uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ba803-b863-452c-a4a1-e6ab9ea8e4d1",
   "metadata": {},
   "source": [
    "Uniformity is all about units, for example celcius to fahrenheits, dates of different formats.\n",
    "\n",
    "So whenever you are working with columns, think if it is possible to have different formats.\n",
    "\n",
    "**Dates:**\n",
    "\n",
    "Just remember that when working with dates, you can always use the .`datetime()` \n",
    "- It accepts any format and return you a uniform format\n",
    "You can do this via:\n",
    "\n",
    "```python\n",
    "df['date_cols'] = pd.to_datetime(df['date_cols'])\n",
    "```\n",
    "\n",
    "Now, this may return an error because of weird formats, what we can do then is have an argument:\n",
    "\n",
    "```python\n",
    "df['date_cols'] = pd.to_datetime(df['date_cols'], errors = 'coerce'\n",
    "# return NA\n",
    "```\n",
    "\n",
    "**Reformatting the dates:**\n",
    "\n",
    "You can use the `df.strftime(\"\")` on a series of dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872cc1b-d782-4f40-a320-1821652095cb",
   "metadata": {},
   "source": [
    "### Cross Field Validation\n",
    "\n",
    "The use of **multiple** fields to sanity check the data integrity\n",
    "\n",
    "One good exammple of this is if you have columns: `age`, `birth_date`\n",
    "\n",
    "You can sanity check the age by subtracting the today's date to the birthdate\n",
    "\n",
    "```python\n",
    "df['birthday'] = pd.to_datetime(users['birthday'])\n",
    "today = dt.date.today()\n",
    "\n",
    "age_manual = today.year - df['birthday'].dt.year\n",
    "\n",
    "age_equ = age_manual == df['age']\n",
    "\n",
    "inconsistent_age = df[~age_equ]\n",
    "\n",
    "consistent_age = df[age_equ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e088c-6b1d-4692-ad4b-8135185d615f",
   "metadata": {},
   "source": [
    "### Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14255051-b4db-4763-a56f-668a4b1582f7",
   "metadata": {},
   "source": [
    "#### Checking For Missing Values\n",
    "- We chain: `.isna()` and `.sum()`\n",
    "\n",
    "Example:\n",
    "`df.isna().sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7031ad-9b21-45cd-a8c9-73418eb60a63",
   "metadata": {},
   "source": [
    "## IV. Record Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44b68a-76a8-4b08-a694-bdb36786ce17",
   "metadata": {},
   "source": [
    "### Comparing strings\n",
    "\n",
    "**Minimum Edit Distance** is a way to measure how different two strings are. It tells us the minimum number of operations needed to convert one string into another.\n",
    "\n",
    "The allowed operations are usually:\n",
    "- **Insertion**: Add a character.\n",
    "- **Deletion**: Remove a character.\n",
    "- **Substitution**: Replace one character with another.\n",
    "\n",
    "#### Example:\n",
    "Convert `\"kitten\"` to `\"sitting\"`:\n",
    "\n",
    "1. kitten → sitten (substitute 'k' with 's')  \n",
    "2. sitten → sittin (substitute 'e' with 'i')  \n",
    "3. sittin → sitting (insert 'g')\n",
    "\n",
    "🟰 Minimum edit distance = **3**\n",
    "\n",
    "The lowest the edit distance the better, that means that the words can be related\n",
    "\n",
    "There are different algorithms to use for this problem for differetn scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2c15a-f476-4661-8971-06f51fdbc845",
   "metadata": {},
   "source": [
    "### Simple String Comparison\n",
    "We can use `thefuzz` to compare within each string, the output of this is from 0 - 100, 100 being two words are similar\n",
    "\n",
    "```python\n",
    "from thefuzz import fuzz\n",
    "\n",
    "fuzz.WRatio('Reeding', 'Reading')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f0146-1c67-4b36-98fc-4677ea384e80",
   "metadata": {},
   "source": [
    "### Comparison with Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce972698-6aa6-453e-8cff-edfde89103e4",
   "metadata": {},
   "source": [
    "You can compare a string to an array of stiring using `thefuzz` \n",
    "\n",
    "```python\n",
    "from thefuzz import process\n",
    "\n",
    "string = 'Housten Rockets vs Los Angeles Lakers'\n",
    "\n",
    "choices = pd.Series([..., ..., ...])\n",
    "\n",
    "process.extract(string, choices, limit = n)\n",
    "```\n",
    "\n",
    "this return a tuple `(matching string, score, index)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45034741-4438-47f2-9ba3-806c446b2200",
   "metadata": {},
   "source": [
    "### Collapsing Categories with String Similarity\n",
    "\n",
    "Now, there will be times when we are presented with a series of different values (perhaps a typo), we can use string similarity to collapse those typos to a single category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1110b34-e91c-455e-9694-c7b33b7ff077",
   "metadata": {},
   "source": [
    "The thought process to this is:\n",
    "\n",
    "1. Get the unique values of the column with typos\n",
    "2. Investigate the lowest score needed for the match with `process.extract('', column)`\n",
    "3. Get the lowest score that matched and use that to transform all the typos\n",
    "\n",
    "### Sample Code:\n",
    "```python\n",
    "from thefuzz import process\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get the unique values of the column with typos\n",
    "typos = [\n",
    "    \"Italy\", \"Iatily\", \"Ityaly\", \"italy\",\n",
    "    \"USA\", \"Usa\", \"us\", \"u.s.a\",\n",
    "    \"Germany\", \"germany\", \"Ger\", \"germ\"\n",
    "]\n",
    "\n",
    "# The correct categories to match against\n",
    "correct_categories = [\"Italy\", \"USA\", \"Germany\"]\n",
    "\n",
    "# Create a DataFrame for demonstration\n",
    "df = pd.DataFrame({\"country_typos\": typos})\n",
    "unique_typos = df[\"country_typos\"].unique()\n",
    "\n",
    "# 2. Investigate the lowest score needed for the match\n",
    "# We'll use a threshold of 75 as an example. You would determine this\n",
    "# by manually checking the scores of various matches.\n",
    "threshold = 75\n",
    "\n",
    "def collapse_category(typo_value):\n",
    "    \"\"\"\n",
    "    Finds the best match for a typo value against the correct categories.\n",
    "    \"\"\"\n",
    "    # process.extract returns a list of tuples: (matched_string, score, index)\n",
    "    best_match = process.extractOne(typo_value, correct_categories)\n",
    "\n",
    "    # If the score is above the threshold, return the best match.\n",
    "    if best_match and best_match[1] >= threshold:\n",
    "        return best_match[0]\n",
    "    else:\n",
    "        # If no match is found, return the original value or None\n",
    "        return typo_value\n",
    "\n",
    "# 3. Transform all the typos\n",
    "df[\"cleaned_country\"] = df[\"country_typos\"].apply(collapse_category)\n",
    "\n",
    "print(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f34fc-86da-4103-983d-f9c6f4fcc8f0",
   "metadata": {},
   "source": [
    "### Record Linkage\n",
    "\n",
    "There is a much better notes about this in a markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ba820-873d-4476-8023-9ff21546f99d",
   "metadata": {},
   "source": [
    "It is the act of linking data from different sources regarding the same entity.\n",
    "\n",
    "To do this:\n",
    "- We generally clean two or more DataFranes\n",
    "- Generate pairs of potentially matching records\n",
    "- Score these pairs according to string similarity metrics\n",
    "- Link them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4baea-ddc7-491c-aff1-432a9c298016",
   "metadata": {},
   "source": [
    "For example. if you have two dataframes of census, they are taken from various sources so you can't simply merge them because there may be duplication.\n",
    "\n",
    "We want to generate pairs from each df (kinda like cartesian product) however we cannot simply do that becasue our dataframe can grow and therefore this combination can scale\n",
    "\n",
    "But what we can do then is look for the matching column (this is called blocking) like state\n",
    "\n",
    "```python\n",
    "import recordlinkage\n",
    "\n",
    "# use to generate pairs from dfs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Generate pairs blocked on state\n",
    "indexer.block('state')\n",
    "\n",
    "pairs = indexer.index(df1, df2)\n",
    "```\n",
    "\n",
    "The output is array of possible pairs of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3470f-201d-4181-9b6c-2ed75c3bf2b1",
   "metadata": {},
   "source": [
    "Once you found _possible_ pairs, it is now time to compare those pairs if they **really** match\n",
    "```python\n",
    "# generate pairs\n",
    "pairs = indexer.index(df1, df2)\n",
    "\n",
    "# creatae a comparing object \n",
    "compare_obj = recordlinkage.Compare()\n",
    "\n",
    "# find exact matches for pairs of col 1 and col 2\n",
    "compare_obj.exact('col1', 'col1', label = 'col1')\n",
    "compare_obj.exact('col2', 'col2', label = 'col2')\n",
    "# these are the columns that you know are exact\n",
    "\n",
    "# find similar matches for pairs of other columns that mau ne be exact\n",
    "compare_obj.string('col_x', 'col_x', threshold = 0.85, label = 'col_x')\n",
    "compare_obj.string('col_y', 'col_y', threshold = 0.85, label = 'col_y')\n",
    "\n",
    "# find matches\n",
    "potential_matches = compare_obj.compute(pairs, df1, df2)\n",
    "```\n",
    "\n",
    "The result of this is a multi-index dataframe, the first index is the row index in the first df and the second index is the list of second index in df2.\n",
    "\n",
    "To find for potential matches, we simpy just sum rows and get the sum whose value exceeds our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9a808-a802-4b38-9daa-a3efa6e5ee37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
