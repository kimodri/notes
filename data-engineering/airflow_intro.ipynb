{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8cc24c-d8aa-4c5b-8912-145af5f238e8",
   "metadata": {},
   "source": [
    "# Airflow DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888dbb19-9459-44a2-aa39-38cf85016861",
   "metadata": {},
   "source": [
    "- Dags are made up of components (task) to be executed such as operators, sensors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0197cb-3c48-42b1-a563-14412a2bc5f9",
   "metadata": {},
   "source": [
    "## Defining the DAG\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from dataetime import datetime\n",
    "\n",
    "defaut_arguments = {\n",
    "    'owner': 'kim',\n",
    "    'email': 'kim@gmail.com',\n",
    "    'start_date': datetime(2020, 1, 20)\n",
    "}\n",
    "# etl_dag = DAG(...)\n",
    "with DAG('etl_workflow', default_args=default_arguments) as etl_dag:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685cb42-ca2d-4d56-a0f2-4b230de10dfc",
   "metadata": {},
   "source": [
    "## Operators\n",
    "- Represent a single task (a class of a task, task is an instance of an operator)\n",
    "  - run a python script\n",
    "  - send an email\n",
    "- They do not share information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0d3a5-d24b-42e6-9ff8-05f78fe81881",
   "metadata": {},
   "source": [
    "### BashOperator\n",
    "- Executes a given bash command or script\n",
    "- Would appear inisde a `with DAG():`\n",
    "- Runs the command in a temporary directory that clean itself up afterwards\n",
    "- **can specify env var for the command (runtime settings provided by the shell)**\n",
    "```python\n",
    "\n",
    "from airflow.operators.bash import BashOperator\n",
    "BashOperator(\n",
    "    task_id = 'bash_example',\n",
    "    bash_command = 'runcleanup.sh', # uses a predefined bash script\n",
    ")\n",
    "\n",
    "# this operator runs a simple cleanup\n",
    "bash_task = BashOperator(\n",
    "    task_id = 'clean_addresses',\n",
    "    bash_command = 'cat addresses.txt | awk \"NF==10\" > cleaned.txt', \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6af80-4bd2-4908-b2d1-9782f554ad97",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7db1c-0e15-4c5f-8a2b-fddaa28351c7",
   "metadata": {},
   "source": [
    "- Instances of operators\n",
    "- Usually assigned to a variable in python\n",
    "\n",
    "```python\n",
    "bash_task = BashOperator(\n",
    "    task_id = 'clean_addresses',\n",
    "    bash_command = 'cat addresses.txt | awk \"NF==10\" > cleaned.txt', \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e37e26-d383-4c87-9a1a-7d88455e5055",
   "metadata": {},
   "source": [
    "### Task Dependencies\n",
    "- Define the order of task completion\n",
    "- AKA upstream or downstream tasks\n",
    "- We used `bitshift` operators to define dependencies\n",
    "  - `>>` upstream (before)\n",
    "  - `<<` downstream (after)\n",
    "  - `task 1 >> task 2`\n",
    "\n",
    "```python\n",
    "task1 = BashOperator(\n",
    "    task_id = 'firs_task',\n",
    "    bash_command = 'echo1',\n",
    ")\n",
    "\n",
    "task2 = BashOperator(\n",
    "    task_id = 'secoond_task',\n",
    "    bash_command = 'echo 2'\n",
    ")\n",
    "\n",
    "task1 >> task2  # task2 << task 1 \n",
    "\n",
    "# You can also do\n",
    "# task1 >> task2\n",
    "# task3 >> task2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208e4ee-3723-4327-9003-94bcd202b61c",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Python Operators\n",
    "- executes a callable (function)\n",
    "- can pass in arguments to the python code\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def printme():\n",
    "    print('something')\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    task_id = 'simple_print',\n",
    "    python_callable=printme\n",
    ")\n",
    "```\n",
    "\n",
    "### You can pass arguments:\n",
    "- `op_kwargs` we use that to use keyword arguments\n",
    "\n",
    "```python\n",
    "def sleep(length_of_time):\n",
    "    time.sleep(length_of_time)\n",
    "\n",
    "sleep_task = PythonOperator(\n",
    "    task_id = 'sleep',\n",
    "    python_callable = sleep,\n",
    "    op_kwargs = {'length_of_time': 5}\n",
    ")\n",
    "```\n",
    "\n",
    "An example of python code operator for data engineering is when **pulling** a data:\n",
    "\n",
    "```python\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")\n",
    "```\n",
    "\n",
    "The next task that you probably want to do is process it and send it thru emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69cb9b-10ee-430c-acb5-626ac74deda9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## Airflow Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c2aff-f87a-44a6-937a-26d4bdc793c6",
   "metadata": {},
   "source": [
    "### DAG Run\n",
    "To understand scheduling, one must know `DAG Run`, that is an instanced of a workflow at a specific time\n",
    "- Could be a workflow runnin now\n",
    "- workflow running yesterday\n",
    "- Can be run **manually** or via `schedule_interval` passed  when creating a DAG\n",
    "  - running\n",
    "  - failed\n",
    "  - success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acca234-6a96-4d52-a676-f9ed94bf2a20",
   "metadata": {},
   "source": [
    "### Airflow DAG Scheduling\n",
    "\n",
    "To control when your DAGs run, configure the following key attributes:\n",
    "\n",
    "- `start_date`:  \n",
    "  - **Required**  \n",
    "  - The date/time from which the DAG starts scheduling (in UTC).\n",
    "  - Example: `datetime(2025, 8, 1, 6, 0)` — starts August 1 at 6 AM UTC\n",
    "\n",
    "- `end_date`:  \n",
    "  - **Optional**  \n",
    "  - The last date for scheduling DAG runs. After this, no new runs will be created.\n",
    "\n",
    "- `max_tries`:  \n",
    "  - **Optional**  \n",
    "  - Limits the number of retries for failed DAG runs.\n",
    "\n",
    "- `schedule_interval`:  \n",
    "  - **Required**  \n",
    "  - Defines how often the DAG runs.\n",
    "  - Accepts:\n",
    "    - **Cron expressions** (e.g., `\"0 12 * * *\"`)  \n",
    "    - **Presets**:\n",
    "      - `\"@once\"` → run only once\n",
    "      - `\"@hourly\"` → every hour\n",
    "      - `\"@daily\"` → every day at midnight\n",
    "      - `\"@weekly\"` → every Sunday at midnight\n",
    "      - `\"@monthly\"` → first day of month at midnight\n",
    "      - `\"@yearly\"` or `\"@annually\"` → Jan 1 at midnight\n",
    "\n",
    "### Cron Syntax\n",
    "\n",
    "┌──────────── minute (0 - 59)\\\n",
    "│ ┌────────── hour (0 - 23)\\\n",
    "│ │ ┌──────── day of month (1 - 31)\\\n",
    "│ │ │ ┌────── month (1 - 12)\\\n",
    "│ │ │ │ ┌──── day of week (0 - 6) (Sunday=0)\\\n",
    "│ │ │ │ │\\\n",
    "│ │ │ │ │\\\n",
    "\\*  *  *  *   *\n",
    "\n",
    "\n",
    "| Example        | Meaning                             |\n",
    "|----------------|-------------------------------------|\n",
    "| `* * * * *`     | Every minute                        |\n",
    "| `0 * * * *`     | Every hour at minute 0              |\n",
    "| `0 12 * * *`    | Every day at 12:00 PM               |\n",
    "| `0 8 * * 1`     | Every Monday at 8:00 AM             |\n",
    "| `30 6 * * 1-5`  | Weekdays (Mon–Fri) at 6:30 AM       |\n",
    "| `15 14 1 * *`   | Every month on the 1st at 2:15 PM   |\n",
    "| `*/10 * * * *`  | Every 10 minutes                    |\n",
    "\n",
    "> Use tools like [crontab.guru](https://crontab.guru) to help visualize schedules.\n",
    "\n",
    "### Sample code:\n",
    "```python\n",
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "    'owner': 'Engineering',\n",
    "    'start_date': datetime(2023, 11, 1),\n",
    "    'email': ['airflowresults@datacamp.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='update_dataflows',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"30 12 * * 3\" , \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f272f1-40b4-4fbb-8643-4a528d3d7aa5",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1315d88-7289-426c-8bcf-ba9a1c806571",
   "metadata": {},
   "source": [
    "## Airflow Sensors\n",
    "\n",
    "Sensors are special Airflow operators that **wait for a condition to be met** before allowing the next task to run. They're used to detect **external triggers**, such as files, HTTP responses, database results, or other DAGs.\n",
    "\n",
    "\n",
    "#### 🔧 Common Sensor Parameters\n",
    "\n",
    "| Parameter       | Description                                        |\n",
    "|-----------------|----------------------------------------------------|\n",
    "| `poke_interval` | How often to check the condition (in seconds)      |\n",
    "| `timeout`       | Max time to wait before failing (in seconds)       |\n",
    "| `mode`          | `'poke'` (default) or `'reschedule'`               |\n",
    "| `soft_fail`     | Mark task as skipped instead of failed on timeout  |\n",
    "\n",
    "\n",
    "### Built-in Sensors\n",
    "\n",
    "| Sensor Name           | Purpose                                       |\n",
    "|-----------------------|-----------------------------------------------|\n",
    "| `FileSensor`          | Wait for a file in the local filesystem       |\n",
    "| `S3KeySensor`         | Wait for a file (key) in an S3 bucket         |\n",
    "| `HttpSensor`          | Wait for an HTTP endpoint to respond          |\n",
    "| `SqlSensor`           | Wait for a SQL query to return a result       |\n",
    "| `ExternalTaskSensor`  | Wait for a task in another DAG to succeed     |\n",
    "\n",
    "\n",
    "###  Example: Wait for File before Proceeding\n",
    "\n",
    "- Checks for the existence of a file at a certain location\n",
    "- checks if any files exists within a directory\n",
    "\n",
    "```python\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "wait_for_file = FileSensor(\n",
    "    task_id='wait_for_input_file',\n",
    "    filepath='/data/input.csv',\n",
    "    poke_interval=60,   # check every 60 seconds\n",
    "    timeout=600,        # stop waiting after 10 minutes\n",
    "    mode='poke'         # or use 'reschedule' for better resource usage\n",
    ")\n",
    "```\n",
    "\n",
    "When do we usually use a sensor?\n",
    "- uncertain when it will be true\n",
    "- if failure not immediately desired\n",
    "- to add task repitition without loops\n",
    "\n",
    "**Task Dependencies vs Sensors**\n",
    "\n",
    "Q: But aren’t all tasks “waiting” for something because of `task1 >> task2`?\n",
    "\n",
    "**Yes** — `task1 >> task2` means `task2` will only run if `task1` succeeds.\n",
    "**However,** this only checks Airflow-internal task success.\n",
    "\n",
    "To wait for something outside Airflow, like:\n",
    "* a file to be dropped\n",
    "* a response from an API\n",
    "* a table to be populated in a database\n",
    "\n",
    "...you need a **sensor** to actively check for that condition.\n",
    "\n",
    "**Task dependencies** handle sequencing,\n",
    "**sensors** handle readiness of external conditions.\n",
    "\n",
    "### Sample code:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG('file_sensor_example', start_date=datetime(2023, 1, 1), schedule_interval='@daily', catchup=False) as dag:\n",
    "    \n",
    "    wait_for_file = FileSensor(\n",
    "        task_id='wait_for_input_file',\n",
    "        filepath='/data/input.csv',\n",
    "        poke_interval=60,\n",
    "        timeout=600,\n",
    "        mode='poke'\n",
    "    )\n",
    "    \n",
    "    process_data = DummyOperator(task_id='process_data')\n",
    "    \n",
    "    wait_for_file >> process_data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6246f5f-a2a7-48e9-a4b7-4a8660c02814",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ed7ab-320b-4f66-a7e9-f98b376bcb32",
   "metadata": {},
   "source": [
    "## Airflow Executors\n",
    "\n",
    "An **executor** in Airflow determines **how and where tasks are executed**. While the scheduler decides *when* tasks should run, the executor is responsible for actually **running** the task instances.\n",
    "\n",
    "\n",
    "### Common Executor Types\n",
    "\n",
    "#### SequentialExecutor\n",
    "- Default executor for Airflow (when first installed)\n",
    "- Runs **only one task at a time**\n",
    "- Not suitable for production or parallel DAGs\n",
    "- Good for learning or simple local testing\n",
    "\n",
    "#### LocalExecutor\n",
    "- Runs tasks **in parallel on the same machine**\n",
    "- Uses Python multiprocessing\n",
    "- Parallelism controlled via:\n",
    "  - `parallelism` in `airflow.cfg`\n",
    "  - `max_active_tasks_per_dag` in DAG settings\n",
    "- Suitable for single-machine production setups\n",
    "\n",
    "#### KubernetesExecutor\n",
    "- Launches each task in a **separate Kubernetes pod**\n",
    "- Highly scalable and cloud-native\n",
    "- Ideal for dynamic, containerized, isolated task environments\n",
    "\n",
    "#### CeleryExecutor *(optional)*\n",
    "- Uses a **distributed task queue** (e.g., Redis/RabbitMQ)\n",
    "- Allows running tasks across **multiple worker nodes**\n",
    "- Good for horizontally scaling across machines\n",
    "\n",
    "\n",
    "### How to Check Which Executor You're Using\n",
    "\n",
    "- Open your `airflow.cfg` file:\n",
    "  ```ini\n",
    "  executor = LocalExecutor\n",
    "  ```\n",
    "- or run this `airflow info`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a31ad8-3128-4e4d-9b62-aa49ee3005ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb1f6f-c062-491a-900d-d75fea8dbf70",
   "metadata": {},
   "source": [
    "## Debugging and Troubleshooting in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfee5b8-6124-42d1-9432-d3c4a9e5774e",
   "metadata": {},
   "source": [
    "### Common problems:\n",
    "1. DAG won't run on scheduler\n",
    "    - Check if scheduler is running in the web\n",
    "    - or fix it easily with `airflow schedule` in the cmd\n",
    "    - There could be a problem with executor\n",
    "3. DAG won't load\n",
    "    - DAG won't appear in web or `airrflow dags list`\n",
    "    - To solve:\n",
    "    - Verify DAG file is in correct folder: `airflow.cg`\n",
    "5. Syntax errors\n",
    "    - To see errors use `airflow dags list-import-errors`\n",
    "  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4d067-3b27-40b9-a1ac-36f3dd31c74f",
   "metadata": {},
   "source": [
    "## SLA (Service Level Agreement) in Airflow\n",
    "\n",
    "In Airflow, an SLA defines the **maximum amount of time** a task should take to complete after the DAG run starts. It is used to monitor whether tasks are running on time.\n",
    "\n",
    "### Defining an SLA\n",
    "\n",
    "An SLA is defined **per task** using the `sla` parameter:\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import timedelta\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='process_data',\n",
    "    python_callable=process_data_func,\n",
    "    sla=timedelta(minutes=30),\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "This means the task is expected to complete within 30 minutes from the start time of the DAG run.\n",
    "\n",
    "### SLA Configuration Rules\n",
    "- The sla parameter must be defined inside the operator, not in default_args. (but you can if you wanna to time the entire workflow)\n",
    "- If defined in default_args, it will be silently ignored.\n",
    "- If a task misses its SLA, it will appear under \"SLA Misses\" in the UI and can trigger email alerts (if configured).\n",
    "\n",
    "### SLA Notifications\n",
    "You can receive notifications for SLA misses using email_on_sla_miss in default_args:\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email': ['alerts@example.com'],\n",
    "    'email_on_sla_miss': True,\n",
    "    'email_on_failure': = True,\n",
    "    'email_on_success': = True,   \n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### SLA vs Task Duration\n",
    "- Tasks like PythonOperator or BashOperator often finish quickly (seconds or milliseconds).\n",
    "- SLA is not based on average runtime, but on how long you're willing to wait before alerting if something is delayed.\n",
    "- Even fast tasks can benefit from SLA monitoring if they are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e50f0-102f-4ecf-a924-5ebbb5fd1741",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Building Production Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da02e8-0576-4442-9554-1602517dcf8a",
   "metadata": {},
   "source": [
    "## Working with Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5de63-9913-45da-92de-f87778447174",
   "metadata": {},
   "source": [
    "- Templates are simply jinja, for example if you want to `echo` differen files you can do it this way:\n",
    "\n",
    "```python\n",
    "task1 = BashOperator(\n",
    "    task_id = 'task_id1'\n",
    "    command = 'echo file1.txt',\n",
    ")\n",
    "task2 = BashOperator(\n",
    "    task_id = 'task_id2'\n",
    "    command = 'echo file2.txt',\n",
    ")\n",
    "```\n",
    "\n",
    "Now that is inefficient because we can have more than 100 files, what you can do then is use jinja:\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "    echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "t1 = BashOperator(\n",
    "    bash_command=templated_coommand,\n",
    "    params = {'filename': 'file1.txt'}\n",
    ")\n",
    "```\n",
    "so to improve the first task:\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "    echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "task1 = BashOperator(\n",
    "    task_id = 'task_id1',\n",
    "    command = templated_command,\n",
    "    params = {'filename': 'task1.txt'}\n",
    ")\n",
    "task2 = BashOperator(\n",
    "    task_id = 'task_id2',\n",
    "    command = templated_command,\n",
    "    params={'filename': 'task2.txt'}\n",
    ")\n",
    "```\n",
    "\n",
    "You can even do `for loops` inside the command and what you put inisde the params is a list.\n",
    "\n",
    "#### Example code of using predefined variables and templates:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf6f2b-5f94-4677-b734-0c3d78ad7870",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sending Templated emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e00b0-1a3e-4103-b476-4ce57d1f71b6",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2023, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6ca29-d5a0-400e-b8cf-11499fdd8c31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Branching\n",
    "- provides conditional logic (`if`)\n",
    "- use BranchPythonOperator: `from airflow.operaots.python import BranchPythonOperator`\n",
    "\n",
    "\n",
    "For example, if you have tasks for even days and odd days, you can create a `branchpythonoperator` that has an `if` and then it will return a string which will be processed by airflow and it expects those strings to be the `string_id` of the task\n",
    "\n",
    "```python\n",
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id = 'branch_task',\n",
    "    dag=dag,\n",
    "    provide_context=True,\n",
    "    python_callable=branch_test\n",
    ")\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2\n",
    "beanch_task >> odd_day_task >> odd_day_task2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c4307-e25e-4ee7-8867-91ca581902f2",
   "metadata": {},
   "source": [
    "> The function returns the task ID of the next task(s) to run. Airflow will skip any downstream tasks not returned.\n",
    "\n",
    "### Q: Why does the function only return a string? How does that decide what runs?\n",
    "Airflow expects the BranchPythonOperator to return a task_id (or list of task_ids). Only those returned tasks will be executed; all other downstream tasks will be marked as skipped.\n",
    "\n",
    "### Q: Why does the function use **kwargs? Shouldn't it use op_kwargs={'key': value}?\n",
    "In standard Python, **kwargs captures named arguments passed to the function. In Airflow, when provide_context=True is used, it automatically injects Airflow’s execution context as **kwargs, including variables like ds, ds_nodash, execution_date, ti, etc. You do not need to manually pass these values.\n",
    "\n",
    "If you want to pass your own custom arguments, you can use op_kwargs, like this:\n",
    "```python\n",
    "BranchPythonOperator(\n",
    "    task_id='custom_branch',\n",
    "    python_callable=my_func,\n",
    "    provide_context = True,\n",
    "    op_kwargs={'threshold': 10}\n",
    ")\n",
    "```\n",
    "Then your function can look like:\n",
    "```python\n",
    "def my_func(threshold, **kwargs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Sample code:\n",
    "```python\n",
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_task >> current_year_task\n",
    "branch_task >> new_year_task\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6b9f3-ffdf-44d7-9208-bed7e076fe72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fb3a5-bfc8-4fea-a923-9141d5888dac",
   "metadata": {},
   "source": [
    "Review:\n",
    "- To run a specific task: `airrflow tasks test <dag_id> <task_id> <date>`\n",
    "- To run a full airflow: `airflow dags trigger -e <date> <dag_id>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69151da7-754d-4350-8bbb-83caa4e035da",
   "metadata": {},
   "source": [
    "### Sample Production Pipeline\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = EmptyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,                                \n",
    "                                   dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644b20b-ad29-4fa9-813b-885fdf1307dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
