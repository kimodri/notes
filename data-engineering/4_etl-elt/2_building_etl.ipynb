{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4885135-d140-4d37-a30b-b6cf73ad93c8",
   "metadata": {},
   "source": [
    "# Extracting Data from a Source System\n",
    "\n",
    "I have written the same thing before, about extracting data notes, you can hover to that to see examples and know how/where to start!\n",
    "- Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d419647-80d4-451e-a717-e9499654e8b9",
   "metadata": {},
   "source": [
    "This module though will talk about getting data from a \n",
    "- csv\n",
    "- Parquet files\n",
    "- JSON files\n",
    "- SQL databases\n",
    "\n",
    "We can also:\n",
    "- API\n",
    "- Data lakes\n",
    "- Data warehouses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c40e97-d0ff-4eef-8ca3-db8d28c780ae",
   "metadata": {},
   "source": [
    "## Reading Parquet files\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('data.parquet', engine='fastparquet')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d1e8c-657f-4686-850a-f0499e344310",
   "metadata": {},
   "source": [
    "## Reading SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97e6c7-1efe-4dee-a78c-9a7e150f042e",
   "metadata": {},
   "source": [
    "To read data to a database a `connection` object has to be created to be connected to a database, this is done using SQLAlchemy create_engine method\n",
    "```python\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "# Connection URI: schema_identifier://username:password@host:port/db\n",
    "connection_uri = 'postgresql+psycopg2://repl:password@localhost:5432/name_of_db'\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "```\n",
    "\n",
    "Now, once the connection is established. You can use that to query:\n",
    "```python\n",
    "data = pd.read_sql(\"SELECT * FROM table LIMIT 10\", db_engine)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb205f-6cbf-43fe-8355-ff8886c8ffa1",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "You must create a function for each letter in ETL:\n",
    "\n",
    "```python\n",
    "def extract_from_sql(connection_uri, query):\n",
    "    db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "extract_from_sql('uri', 'query')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912ffda-2eb7-4536-8ddb-18516f24f199",
   "metadata": {},
   "source": [
    "# Transforming Data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef9178-ea2d-4fe9-bd15-5bcc372e5d62",
   "metadata": {},
   "source": [
    "- You can filter data with `loc` and `iloc`\n",
    "- You can also alter datatypes\n",
    "    - `cleaned['timestamps'] = pd.to_datetime(cleaned['timestamps'], format = '%Y%m%d%H%M%S`)\n",
    "    - `cleaned['timestamps'] = pd.to_datetime(cleaned['timestamps'], unit = 'ms')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9314e4-2bdf-4ae7-872d-bb20801c8d84",
   "metadata": {},
   "source": [
    "When you transform **you must validate transformation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee64e7-314d-4e58-b799-1d845bf429ce",
   "metadata": {},
   "source": [
    "## Example of Extracting and Transforming: Use Case\n",
    "\n",
    "```python\n",
    "def extract(file_path):\n",
    "    raw_data = pd.read_parquet(file_path)\n",
    "    return raw_data\n",
    "\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Filter rows and columns\n",
    "    clean_data = raw_data.loc[raw_data['Quantity Ordered'] == 1, ['Order ID', 'Price Each', 'Quantity Ordered']]\n",
    "    return clean_data\n",
    "\n",
    "# Transform the raw_sales_data\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129efb0-e221-4e7e-8e29-aedf4699a01e",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f8fdd-2839-473b-ad13-816a62cdc6c5",
   "metadata": {},
   "source": [
    "Persisting data allows for a snapshot of the data.\n",
    "\n",
    "This is easy with `to_csv()` method:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# data extraction and transformation\n",
    "raw_data = pd.read_csv('data.csv')\n",
    "\n",
    "transformed_data = raw_data.loc[raw_data['col'] > 100, ['col2', 'col3']]\n",
    "\n",
    "# load dta\n",
    "loaded_data.to_csv('loaded_data.csv')\n",
    "```\n",
    "\n",
    "There are arguments to customize the loaded csv files:\n",
    "- header\n",
    "- index\n",
    "- sep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3c21d-b09f-4e57-a507-4cd05ff3119e",
   "metadata": {},
   "source": [
    "## Ensuring Data Persistence\n",
    "How do we know that what we just loaded is correct?\n",
    "- We can check the filepath using OS module\n",
    "\n",
    "```python\n",
    "# contonuing from the above\n",
    "\n",
    "file_exists = os.path.exists('loaded_data.csv')\n",
    "print(file_exists)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5b6b5-1811-49ef-95e6-cfe8e55a0b6f",
   "metadata": {},
   "source": [
    "# Monitor Pipelines\n",
    "- Pipelines should be monitored due to data or failures\n",
    "- Sometimes package can be depracated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765694dc-56df-4eec-92e9-83f2d1e714c2",
   "metadata": {},
   "source": [
    "## Logging Data Pipeline Performance\n",
    "- This is just documentung performance at execution\n",
    "- You can use logging module\n",
    "\n",
    "The following are the methods you can use from the said module to successfully log your performances/works:\n",
    "\n",
    "**Functions**\n",
    "\n",
    "| Functions | Description | Syntax |\n",
    "|---|---|---|\n",
    "| `logging.debug` | Logs messages that are typically used during development for detailed information. | `logging.debug(message)` |\n",
    "| `logging.info` | Logs messages that provide general information about the pipeline's execution. | `logging.info(message)` |\n",
    "| `logging.warning` | Logs messages about unexpected events that don't necessarily stop the pipeline. | `logging.warning(message)` |\n",
    "| `logging.error` | Logs messages about errors that have occurred and might halt the pipeline. | `logging.error(message)` |\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "| Arguments | Description | Syntax | Example Values |\n",
    "|---|---|---|---|\n",
    "| `message` | The string message to be logged. | `logging.debug(message)`, `logging.info(message)`, `logging.warning(message)`, `logging.error(message)` | `\"Data dimensionality: (100, 5)\"`, `\"Starting data transformation\"`, `\"Unexpected number of rows: 95\"`, `\"KeyError: 'price_change'\"` |\n",
    "| `alias` | A name to refer to the caught exception within the `except` block. | `except SpecificError as alias:` | `e`, `err`, `key_error` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d81cd-0248-424c-87f4-731a0431d381",
   "metadata": {},
   "source": [
    "## Sample Logs\n",
    "Logs provide a starting point when something fails as they tell Data Engineers something about the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79c156-c018-4262-9977-6c21defc465a",
   "metadata": {},
   "source": [
    "```python\n",
    "# sample with debug and info\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(levelname)s: %(message)s', level = logging.DEBUG)\n",
    "\n",
    "# create different types\n",
    "logging.debug(f\"Variable has value {path}.\")\n",
    "\n",
    "# this is just telling co-engineers\n",
    "logging.info(f\"Data has been transformed and will now be loaded.\")\n",
    "```\n",
    "\n",
    "**Warnings and Errors** should also be captured using logs!\n",
    "\n",
    "- **Warning** is used when something unepected happened but an exception has not occured (ex: unexpected number of rows)\n",
    "- **Error** logs are used when an exception occurs that should halt the execution of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84770ae1-ce68-4a17-ab45-eb9855e19ce9",
   "metadata": {},
   "source": [
    "### Handling exceptions with try-except\n",
    "- When an error occured under try rather than ending, the code in th except block will be triggered\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # code in nyah\n",
    "    pass\n",
    "except:\n",
    "    # logging about failures that occured\n",
    "    # logic to execute upon exeption\n",
    "    pass\n",
    "```\n",
    "\n",
    "You already know this, Kim, Java has this. If you know the error, it should be passed next to the except keyword. For example:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    data = transform(data)\n",
    "    logging.info(\"Successfully filtered DataFrame by ... \")\n",
    "\n",
    "except KeyError as ke:\n",
    "    # handle the error cretae a bew column, transform\n",
    "    logging.warning(f\"{ke}: Cannot filter DataFrame by ...\")\n",
    "    data['newCol'] = data['oldCol'] - data['oldCol1']\n",
    "    data = transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705bb05-5efe-4a32-8ca9-1eb2f8ce25c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
