{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68408ab0-e0ce-41d5-bb54-20e6784e9369",
   "metadata": {},
   "source": [
    "# Data Pipelines\n",
    "- Responsible for moving data from a source to a destination and transforming it somewhere along the way\n",
    "    - So you pulled data and transform it\n",
    "- This is what you create and maintain as a Data Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4533f-dfc6-4e04-a6fe-cb106e4163d7",
   "metadata": {},
   "source": [
    "# ETL \n",
    "- A flavor of Data Pipeline\n",
    "- First extract data and transform it before loading it into a destination\n",
    "- Sources maybe tabular or non-tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d5d06-bb61-48d9-ac13-dda08ac035a0",
   "metadata": {},
   "source": [
    "Sample ETL in Python:\n",
    "```python\n",
    "def load(data_frame, traget_table):\n",
    "    # some custom-built python logic to load data to SQL\n",
    "    data_frame.to_sql(name=target_table, con=POSTGRES_CONNECTION)\n",
    "    printf(f\"Loading data to the {target_table} table\")\n",
    "\n",
    "# Now, run the data pipeline\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "load(data_frame=transformed_data, taregt_table=\"cleaned_data\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "Extracting data from raw_data.csv\n",
    "\n",
    "Transforming data to remove 'null' records\n",
    "\n",
    "Loading data to the cleaned_data table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995bec5-c7b2-47fc-ad7f-b36d23622b1d",
   "metadata": {},
   "source": [
    "# ELT \n",
    "- Extracts and load the data before transforming it\n",
    "- More recent because of Data warehouses\n",
    "- typically a tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae976e5-e0e4-44bc-b8a9-e2f4c32d2a1a",
   "metadata": {},
   "source": [
    "Sample ELT:\n",
    "\n",
    "```python\n",
    "def transform(source_table, target_table):\n",
    "    data_warehouse.run_sql(\n",
    "    \"\"\"\n",
    "    CREATE TABLE {target_table} AS\n",
    "        SELECT\n",
    "            <field-name>, <field-name>, ...\n",
    "        FROM {source_table};\n",
    "    \"\"\")\n",
    "# Similar to ETL pipelines, call the extract, load, and transform functions\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_data\")\n",
    "transform(source_table=\"raw_data\", target_table=\"cleaned_data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c99dd-c333-4814-8269-eb693246a683",
   "metadata": {},
   "source": [
    "# Building ETL and ELT Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c28086-a9b5-4981-be82-6bb5af972259",
   "metadata": {},
   "source": [
    "- First we need to be able to pull data from a csv\n",
    "\n",
    "## PULL:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.read_csv('raw_data.csv')\n",
    "```\n",
    "\n",
    "## TRANSFORM:\n",
    "Once we extracted data we can begin to filter (you are just cleaning this, Kim, `loc` and `iloc` lang\n",
    "\n",
    "## LOAD:\n",
    "If you want to load the dataframe to a .csv file, you can do:\n",
    "\n",
    "```python\n",
    "data_frame.to_csv(\"cleaned_data.csv\")\n",
    "```\n",
    "\n",
    "There are also `to_sql()`, `to_json()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff300c67-5dbf-41b6-bdd7-d4bf2a505086",
   "metadata": {},
   "source": [
    "## Using SQL when Transforming:\n",
    "Sometimes we have to use SQL when transforming from a datawarehouse:\n",
    "\n",
    "```python\n",
    "data_warehouse.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE total_sales AS\n",
    "        SELECT\n",
    "            ds,\n",
    "            SUM(sales),\n",
    "        FROM raw_sales_data\n",
    "        GROUP BY ds;\n",
    "    \"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "We can use sqlAlchemy here oy snowflakes connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144588a-c04e-4a72-8bc9-0f1ed21b2965",
   "metadata": {},
   "source": [
    "## Putting it all together:\n",
    "```python\n",
    "# Define extract(), transform(), load() functions\n",
    "\n",
    "def transform(data_frame, value):\n",
    "    return data_frame.loc[data_frame['name']] == value, ['name', 'num_firms']]\n",
    "\n",
    "# first, extract data from .csv\n",
    "extracted_data = extract(file_name='raw_data.csv')\n",
    "\n",
    "# then, transform the 'extracted_data'\n",
    "transformed_data = transform(data_frame = extracted_data, value = 'Apparel')\n",
    "\n",
    "# finally, load the transformed data\n",
    "load(data_frame=transformed_data, file_name='cleaned_data.csv')\n",
    "```\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "def extract(file_name):\n",
    "  return pd.read_csv(file_name)\n",
    "\n",
    "def transform(data_frame):\n",
    "  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n",
    "\n",
    "def load(data_frame, file_name):\n",
    "  data_frame.to_csv(file_name)\n",
    "  \n",
    "extracted_data = extract(file_name=\"raw_industry_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Pass the transformed_data DataFrame to the load() function\n",
    "load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd713d-7a34-472e-882d-067f63d5ae15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
