{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c7a1e6-0d31-4d50-8cba-57ee0055cf31",
   "metadata": {},
   "source": [
    "## Manually Testing Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b03f9a-343e-49e8-b711-53a2a3539ff5",
   "metadata": {},
   "source": [
    "This is done to:\n",
    "- validatae that data is extracted, transformed, loaded as expected\n",
    "- identify and fix data quality issues\n",
    "- improves data reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f838147-d018-45bb-ba90-00d2b1027851",
   "metadata": {},
   "source": [
    "### End-to-End Testing\n",
    "This is doing ETL to a testing environment:\n",
    "- confirms that pipeline runs on repeated attempts\n",
    "- validate data pipeline checkpoints\n",
    "  - This is just having checkpoints every after component of an ETL\n",
    "    ```python\n",
    "    # in loading data, we can check if the loaded data is correct by doing\n",
    "    loaded_data = pd.read_sql(query, engine)\n",
    "    print(my_df.equals(loaded__data))\n",
    "    ```\n",
    "  - You can check what happens after transforming the cleaned data and compare to the extratced raw data\n",
    "- engage in peer review, incorporate feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78949b35-eb1b-4564-bfb3-ba761c2cdf42",
   "metadata": {},
   "source": [
    "An example of manual testing is like this:\n",
    "\n",
    "```python\n",
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "    \n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")\n",
    "```\n",
    "This ensures that the shapes are the same and that the piepline does not change even efter repeating the calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b02b19-3d1b-44a6-8f83-16887aeec234",
   "metadata": {},
   "source": [
    "## Unit Testing a Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95ea9b-ffb8-4c36-8f4f-094b345d5b78",
   "metadata": {},
   "source": [
    "Typical testing in data engineering follows:\n",
    "\n",
    "Unit testing should run -> end-to-end testing -> deploy\n",
    "\n",
    "to do this we use `pytest`\n",
    "\n",
    "```python\n",
    "from pipeline import extract, transform, load\n",
    "\n",
    "# build a test asseting the type of the resulting data after transforming\n",
    "def test_transformed_date():\n",
    "    raw_df = extract('raw.csv')\n",
    "    clean = transform(raw_df)\n",
    "\n",
    "    assert isinstance(clean, pd.DataFrame)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4161e-af27-4eb0-a24c-a901d8f46a93",
   "metadata": {},
   "source": [
    "### Using Fixtures\n",
    "\n",
    "When testing pipelines, we often reuse the same sample or mock data in multiple test cases.\n",
    "\n",
    "To avoid repeating setup code, `pytest` provides **fixtures**, which are functions that return a fixed baseline input or environment for the tests to run.\n",
    "\n",
    "#### What is a fixture?\n",
    "\n",
    "A fixture is a function decorated with `@pytest.fixture`.\n",
    "\n",
    "When a test function includes a parameter with the same name as a fixture, `pytest` automatically calls the fixture and passes its return value to the test function.\n",
    "\n",
    "#### Example: Testing a Data Pipeline with a Fixture\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from pipeline import transform\n",
    "\n",
    "# Define a fixture named 'raw_data'\n",
    "@pytest.fixture\n",
    "def raw_data():\n",
    "    data = {\n",
    "        'date': ['2024-01-01', '2024-01-02'],\n",
    "        'value': [10, 20]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# This test function uses the 'raw_data' fixture\n",
    "def test_transform_output_type(raw_data):\n",
    "    # raw_data is automatically passed in by pytest\n",
    "    result = transform(raw_data)\n",
    "\n",
    "    # Assert that the result is a DataFrame\n",
    "    assert isinstance(result, pd.DataFrame)\n",
    "\n",
    "# Another test using the same raw_data fixture\n",
    "def test_transform_values(raw_data):\n",
    "    result = transform(raw_data)\n",
    "\n",
    "    # For example, check if a 'normalized_value' column was added\n",
    "    assert 'normalized_value' in result.columns\n",
    "```\n",
    "\n",
    "In simpler terms, using a fixture like:\n",
    "\n",
    "```python\n",
    "@pytest.fixture\n",
    "def raw_data():\n",
    "    return pd.DataFrame({...})\n",
    "\n",
    "def test_something(raw_data):\n",
    "    # raw_data is already the DataFrame returned by raw_data()\n",
    "    ...\n",
    "```\n",
    "\n",
    "is like writing:\n",
    "\n",
    "```python\n",
    "def test_something():\n",
    "    raw_data = raw_data()  # You'd need to define raw_data() somewhere\n",
    "```\n",
    "Because in `pytest`, you don’t need to call raw_data() yourself — pytest does that for you automatically, based on the parameter name in the test function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ad5b6-f556-4e62-a2ce-189fe6842347",
   "metadata": {},
   "source": [
    "### Unit Testing on Data Contents\n",
    "You don't only check for the entire dataframe. You also check for contents like the number of columns or the data ranges:\n",
    "\n",
    "```python\n",
    "def test_transformed_data(clean_data):\n",
    "    # check the number of columns\n",
    "    assert len(clean_data.columns) == 4\n",
    "\n",
    "    # check the lowerbound of a column\n",
    "    assert clean_data['col'].min() >= 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df686d4-9b20-47e5-9681-e54ff58b187e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beaf2ba-3531-45ac-9462-ef65bf6d5e5b",
   "metadata": {},
   "source": [
    "## Data Pipeline Architecture Patterns\n",
    "We may have a single file called `eeitl_pipeline.py` that includes the definitions of the E, T, and L and the execution.\n",
    "\n",
    "However, that is not the best implementation, we may want to isolate the definiiton to other file:\n",
    "\n",
    "`>ls`\n",
    "- `etl_pipeline.py`\n",
    "- `pipeline_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b06f73-d174-459d-a319-add719b8b6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
